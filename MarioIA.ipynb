{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MarioIA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-XWsmCfT7nt"
      },
      "source": [
        "import os\n",
        "!pip install tqdm retrowrapper gym-retro\n",
        "os.chdir(\"/usr/local/lib/python3.6/dist-packages/\")\n",
        "!git clone https://github.com/openai/baselines.git\n",
        "!git clone https://github.com/openai/retro-contest.git\n",
        "os.chdir(\"/usr/local/lib/python3.6/dist-packages/baselines\")\n",
        "!pip install e .\n",
        "\n",
        "os.chdir(\"/usr/local/lib/python3.6/dist-packages/retro-contest/support\")\n",
        "!pip install e ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVin8kBxk-_i"
      },
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "os.chdir(\"/content\")\n",
        "!ls\n",
        "!mkdir -p data/roms/\n",
        "print(\"upload roms\")\n",
        "os.chdir(\"data/roms/\")\n",
        "files.upload()\n",
        "!ls\n",
        "os.chdir(\"/content\")\n",
        "print(\"upload models\")\n",
        "files.upload()\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPtSGK4QfeHe"
      },
      "source": [
        "!python3 -m retro.import data/roms/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bPfYaUTQPnG"
      },
      "source": [
        "import retro\n",
        "for game in retro.data.list_games():\n",
        "    print(game, retro.data.list_states(game))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CF6ix-KjZm72"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "# Part taken from adborghi fantastic implementation\n",
        "# https://github.com/aborghi/retro_contest_agent/blob/master/fastlearner/ppo2ttifrutti_sonic_env.py\n",
        "import numpy as np\n",
        "import gym\n",
        "import os\n",
        "\n",
        "#import gym_remote.client as grc\n",
        "#import retrowrapper\n",
        "#from retro_contest.local import make\n",
        "from retro import make as make_retro\n",
        "\n",
        "# This will be useful for stacking frames\n",
        "from baselines.baselines.common.atari_wrappers import FrameStack\n",
        "\n",
        "# Library used to modify frames (former times we used matplotlib)\n",
        "import cv2\n",
        "\n",
        "#import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "\n",
        "# This function selects the probability distribution over actions\n",
        "from baselines.common.distributions import make_pdtype\n",
        "\n",
        "import time\n",
        "#import os.path as osp\n",
        "\n",
        "from baselines import logger\n",
        "\n",
        "#import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate cross entropy\n",
        "from baselines.a2c.utils import cat_entropy\n",
        "\n",
        "from baselines.common import explained_variance\n",
        "from baselines.baselines.common.runners import AbstractEnvRunner\n",
        "import math\n",
        "\n",
        "# SubprocVecEnv creates a vector of n environments to run them simultaneously.\n",
        "from baselines.common.vec_env.subproc_vec_env import SubprocVecEnv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctX0hm19yQ2b"
      },
      "source": [
        "# setUseOpenCL = False means that we will not use GPU (disable OpenCL acceleration)\n",
        "cv2.ocl.setUseOpenCL(True)\n",
        "\n",
        "class PreprocessFrame(gym.ObservationWrapper):\n",
        "  \"\"\"\n",
        "  Here we do the preprocessing part:\n",
        "  - Set frame to gray\n",
        "  - Resize the frame to 96x96x1\n",
        "  \"\"\"\n",
        "  def __init__(self, env):\n",
        "    gym.ObservationWrapper.__init__(self, env)\n",
        "    self.width = 80\n",
        "    self.height = 80\n",
        "    self.observation_space = gym.spaces.Box(low=0, high=255, shape=(self.height, self.width, 1), dtype=np.uint8)\n",
        "\n",
        "  def observation(self, frame):\n",
        "    # Set frame to gray\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    # Resize the frame to 96x96x1\n",
        "    frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
        "    frame = frame[:, :, None]\n",
        "\n",
        "    return frame\n",
        "\n",
        "class ActionsDiscretizer(gym.ActionWrapper):\n",
        "  \"\"\"\n",
        "  Wrap a gym-retro environment and make it use discrete\n",
        "  actions for the Sonic game.\n",
        "  \"\"\"\n",
        "  def __init__(self, env):\n",
        "    super(ActionsDiscretizer, self).__init__(env)\n",
        "    buttons = [\"B\", \"A\", \"MODE\", \"START\", \"UP\", \"DOWN\", \"LEFT\", \"RIGHT\", \"C\", \"Y\", \"X\", \"Z\"]\n",
        "    #actions = [['LEFT'], ['RIGHT'], ['LEFT', 'DOWN'], ['RIGHT', 'DOWN'], ['DOWN'],\n",
        "              #['DOWN', 'B'], ['B']]\n",
        "    actions = [['RIGHT'], ['RIGHT','C'], ['C']]\n",
        "    self._actions = []\n",
        "\n",
        "    \"\"\"\n",
        "    What we do in this loop:\n",
        "    For each action in actions\n",
        "      - Create an array of 12 False (12 = nb of buttons)\n",
        "      For each button in action: (for instance ['LEFT']) we need to make that left button index = True\n",
        "          - Then the button index = LEFT = True\n",
        "\n",
        "      In fact at the end we will have an array where each array is an action and each elements True of this array\n",
        "      are the buttons clicked.\n",
        "    \"\"\"\n",
        "    for action in actions:\n",
        "      arr = np.array([False] * 12)\n",
        "      for button in action:\n",
        "        arr[buttons.index(button)] = True\n",
        "      self._actions.append(arr)\n",
        "    self.action_space = gym.spaces.Discrete(len(self._actions))\n",
        "\n",
        "  def action(self, a): # pylint: disable=W0221\n",
        "    return self._actions[a].copy()\n",
        "\n",
        "class RewardScaler(gym.RewardWrapper):\n",
        "  \"\"\"\n",
        "  Bring rewards to a reasonable scale for PPO.\n",
        "  This is incredibly important and effects performance\n",
        "  drastically.\n",
        "  \"\"\"\n",
        "  def reward(self, reward):\n",
        "    return reward*0.01\n",
        "\n",
        "class AllowBacktracking(gym.Wrapper):\n",
        "  \"\"\"\n",
        "  Use deltas in max(X) as the reward, rather than deltas\n",
        "  in X. This way, agents are not discouraged too heavily\n",
        "  from exploring backwards if there is no way to advance\n",
        "  head-on in the level.\n",
        "  \"\"\"\n",
        "  def __init__(self, env):\n",
        "    super(AllowBacktracking, self).__init__(env)\n",
        "    self._cur_x = 0\n",
        "    self._max_x = 0\n",
        "    self.lives = 2\n",
        "\n",
        "  def reset(self, **kwargs): # pylint: disable=E0202\n",
        "    self._cur_x = 0\n",
        "    self._max_x = 0\n",
        "    return self.env.reset(**kwargs)\n",
        "\n",
        "  def step(self, action): # pylint: disable=E0202\n",
        "    obs, rew, done, info = self.env.step(action)\n",
        "    #self._cur_x += rew\n",
        "    #rew = max(0, self._cur_x - self._max_x)\n",
        "    #self._max_x = max(self._max_x, self._cur_x)\n",
        "    if(info[\"lives\"] < self.lives or done == True):\n",
        "      self.lives = info[\"lives\"]\n",
        "      rew = -2\n",
        "    return obs, rew, done, info\n",
        "\n",
        "\n",
        "def make_env(env_idx): \n",
        "  \"\"\"\n",
        "  Create an environment with some standard wrappers.\n",
        "  \"\"\"\n",
        "  dicts = [{'game': 'SuperMarioBros-Nes', 'state': 'Level1-1'},\n",
        "          {'game': 'SuperMarioBros-Nes', 'state': 'Level2-1'},\n",
        "          {'game': 'SuperMarioBros-Nes', 'state': 'Level3-1'},\n",
        "          {'game': 'SuperMarioBros-Nes', 'state': 'Level4-1'},\n",
        "          {'game': 'SuperMarioBros-Nes', 'state': 'Level5-1'},\n",
        "          {'game': 'SuperMarioBros-Nes', 'state': 'Level6-1'},\n",
        "          {'game': 'SuperMarioBros-Nes', 'state': 'Level7-1'},\n",
        "          {'game': 'SuperMarioBros-Nes', 'state': 'Level8-1'}]\n",
        "      \n",
        "  # Make the environment\n",
        "  print(dicts[env_idx]['game'], dicts[env_idx]['state'], flush=True)\n",
        "  #record_path = \"./records/\" + dicts[env_idx]['state']\n",
        "  env = make_retro(game=dicts[env_idx]['game'], state=dicts[env_idx]['state'], record='.')\n",
        "\n",
        "  # Build the actions array, \n",
        "  env = ActionsDiscretizer(env)\n",
        "\n",
        "  # Scale the rewards\n",
        "  env = RewardScaler(env)\n",
        "\n",
        "  # PreprocessFrame\n",
        "  env = PreprocessFrame(env)\n",
        "\n",
        "  # Stack 4 frames\n",
        "  env = FrameStack(env, 4)\n",
        "\n",
        "  # Allow back tracking that helps agents are not discouraged too heavily\n",
        "  # from exploring backwards if there is no way to advance\n",
        "  # head-on in the level.\n",
        "  env = AllowBacktracking(env)\n",
        "\n",
        "  return env\n",
        "\n",
        "def make_train_0():\n",
        "  return make_env(0)\n",
        "\n",
        "def make_train_1():\n",
        "  return make_env(1)\n",
        "\n",
        "def make_train_2():\n",
        "  return make_env(2)\n",
        "\n",
        "def make_train_3():\n",
        "  return make_env(3)\n",
        "\n",
        "def make_train_4():\n",
        "  return make_env(4)\n",
        "\n",
        "def make_train_5():\n",
        "  return make_env(5)\n",
        "\n",
        "def make_train_6():\n",
        "  return make_env(6)\n",
        "\n",
        "def make_train_7():\n",
        "  return make_env(7)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHix4ey-0TzE"
      },
      "source": [
        "# Convolution layer\n",
        "def conv_layer(inputs, filters, kernel_size, strides, gain=1.0):\n",
        "    return tf.layers.conv2d(inputs=inputs, filters=filters, kernel_size=kernel_size,strides=(strides, strides),\n",
        "                            activation=tf.nn.relu, kernel_initializer=tf.orthogonal_initializer(gain=gain))\n",
        "\n",
        "# Fully connected layer\n",
        "def fc_layer(inputs, units, activation_fn=tf.nn.relu, gain=1.0):\n",
        "    return tf.layers.dense(inputs=inputs, units=units, activation=activation_fn,\n",
        "                           kernel_initializer=tf.orthogonal_initializer(gain))\n",
        "\n",
        "class A2CPolicy(object):\n",
        "  \"\"\"\n",
        "  This object creates the A2C Network architecture\n",
        "  \"\"\"\n",
        "  def __init__(self, sess, ob_space, action_space, nbatch, nsteps, reuse = False):\n",
        "    # This will use to initialize our kernels\n",
        "    gain = np.sqrt(2)\n",
        "\n",
        "    # Based on the action space, will select what probability distribution type\n",
        "    # we will use to distribute action in our stochastic policy (in our case DiagGaussianPdType\n",
        "    # aka Diagonal Gaussian, 3D normal distribution\n",
        "    self.pdtype = make_pdtype(action_space)\n",
        "\n",
        "    height, weight, channel = ob_space.shape\n",
        "    ob_shape = (height, weight, channel)\n",
        "\n",
        "    # Create the input placeholder\n",
        "    inputs_ = tf.placeholder(tf.float32, [None, *ob_shape], name=\"input\")\n",
        "\n",
        "    # Normalize the images\n",
        "    scaled_images = tf.cast(inputs_, tf.float32) / 255.\n",
        "\n",
        "    \"\"\"\n",
        "    Build the model\n",
        "    3 CNN for spatial dependencies\n",
        "    Temporal dependencies is handle by stacking frames\n",
        "    (Something funny nobody use LSTM in OpenAI Retro contest)\n",
        "    1 common FC\n",
        "    1 FC for policy\n",
        "    1 FC for value\n",
        "    \"\"\"\n",
        "    with tf.variable_scope(\"model\", reuse = reuse):\n",
        "      conv1 = conv_layer(scaled_images, 32, 8, 4, gain)\n",
        "      conv2 = conv_layer(conv1, 64, 4, 2, gain)\n",
        "      conv3 = conv_layer(conv2, 64, 3, 1, gain)\n",
        "      flatten1 = tf.layers.flatten(conv3)\n",
        "      fc_common = fc_layer(flatten1, 512, gain=gain)\n",
        "\n",
        "      # This build a fc connected layer that returns a probability distribution\n",
        "      # over actions (self.pd) and our pi logits (self.pi).\n",
        "      self.pd, self.pi = self.pdtype.pdfromlatent(fc_common, init_scale=0.01)\n",
        "      \n",
        "      # Calculate the v(s)\n",
        "      vf = fc_layer(fc_common, 1, activation_fn=None)[:, 0]\n",
        "\n",
        "    self.initial_state = None\n",
        "\n",
        "    # Take an action in the action distribution (remember we are in a situation\n",
        "    # of stochastic policy so we don't always take the action with the highest probability\n",
        "    # for instance if we have 2 actions 0.7 and 0.3 we have 30% chance to take the second)\n",
        "    a0 = self.pd.sample()\n",
        "\n",
        "    # Function use to take a step returns action to take and V(s)\n",
        "    def step(state_in, *_args, **_kwargs):\n",
        "      action, value = sess.run([a0, vf], {inputs_: state_in})\n",
        "      #print(\"step\", action)\n",
        "      return action, value\n",
        "\n",
        "    # Function that calculates only the V(s)\n",
        "    def value(state_in, *_args, **_kwargs):\n",
        "      return sess.run(vf, {inputs_: state_in})\n",
        "\n",
        "    # Function that output only the action to take\n",
        "    def select_action(state_in, *_args, **_kwargs):\n",
        "      return sess.run(a0, {inputs_: state_in})\n",
        "\n",
        "    self.inputs_ = inputs_\n",
        "    self.vf = vf\n",
        "    self.step = step\n",
        "    self.value = value\n",
        "    self.select_action = select_action\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6vEp3eR08_k"
      },
      "source": [
        "# Get the variables\n",
        "def find_trainable_variables(key):\n",
        "  with tf.variable_scope(key):\n",
        "    return tf.trainable_variables()\n",
        "\n",
        "# Make directory\n",
        "def make_path(f):\n",
        "  # exist_ok: if the folder already exist makes no exception error\n",
        "  return os.makedirs(f, exist_ok=True)\n",
        "\n",
        "def discount_with_dones(rewards, dones, gamma):\n",
        "  discounted = []\n",
        "  r = 0\n",
        "  for reward, done in zip(rewards[::-1], dones[::-1]):\n",
        "    r = reward + gamma*r*(1.-done) # fixed off by one bug\n",
        "    discounted.append(r)\n",
        "  return discounted[::-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_E2rjVoG0hWS"
      },
      "source": [
        "class Model(object):\n",
        "  \"\"\"\n",
        "  We use this object to :\n",
        "  __init__:\n",
        "  - Creates the step_model\n",
        "  - Creates the train_model\n",
        "\n",
        "  train():\n",
        "  - Make the training part (feedforward and retropropagation of gradients)\n",
        "\n",
        "  save/load():\n",
        "  - Save load the model\n",
        "  \"\"\"\n",
        "  def __init__(self, policy, ob_space, action_space, nenvs, nsteps, ent_coef, vf_coef, max_grad_norm):\n",
        "    sess = tf.get_default_session()\n",
        "\n",
        "    # Here we create the placeholders\n",
        "    actions_ = tf.placeholder(tf.int32, [None], name=\"actions_\")\n",
        "    advantages_ = tf.placeholder(tf.float32, [None], name=\"advantages_\")\n",
        "    rewards_ = tf.placeholder(tf.float32, [None], name=\"rewards_\")\n",
        "    lr_ = tf.placeholder(tf.float32, name=\"learning_rate_\")\n",
        "\n",
        "    # Here we create our two models:\n",
        "    # Step_model that is used for sampling\n",
        "    step_model = policy(sess, ob_space, action_space, nenvs, 1, reuse=False)\n",
        "\n",
        "    # Train model for training\n",
        "    train_model = policy(sess, ob_space, action_space, nenvs*nsteps, nsteps, reuse=True)\n",
        "\n",
        "    \"\"\"\n",
        "    Calculate the loss\n",
        "    Total loss = Policy gradient loss - entropy * entropy coefficient + Value coefficient * value loss\n",
        "    \"\"\"\n",
        "    # Policy loss\n",
        "    # Output -log(pi)\n",
        "    neglogpac = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=train_model.pi, labels=actions_)\n",
        "\n",
        "    # 1/n * sum A(si,ai) * -logpi(ai|si)\n",
        "    pg_loss = tf.reduce_mean(advantages_ * neglogpac)\n",
        "\n",
        "    # Value loss 1/2 SUM [R - V(s)]^2   \n",
        "    vf_loss = tf.reduce_mean(tf.square(train_model.vf-rewards_))\n",
        "\n",
        "    # Entropy is used to improve exploration by limiting the premature convergence to suboptimal policy.\n",
        "    entropy = tf.reduce_mean(train_model.pd.entropy())\n",
        "\n",
        "    loss = pg_loss - entropy * ent_coef + vf_loss * vf_coef\n",
        "    print(pg_loss)\n",
        "    print(loss)\n",
        "    \n",
        "    # Update parameters using loss\n",
        "    # 1. Get the model parameters\n",
        "    params = find_trainable_variables(\"model\")\n",
        "\n",
        "    # 2. Calculate the gradients\n",
        "    grads = tf.gradients(loss, params)\n",
        "    if max_grad_norm is not None:\n",
        "      # Clip the gradients (normalize)\n",
        "      grads, grad_norm = tf.clip_by_global_norm(grads, max_grad_norm)\n",
        "    grads = list(zip(grads, params))\n",
        "    # zip aggregate each gradient with parameters associated\n",
        "    # For instance zip(ABCD, xyza) => Ax, By, Cz, Da\n",
        "\n",
        "    # 3. Build our trainer\n",
        "    trainer = tf.train.RMSPropOptimizer(learning_rate=lr_, decay=0.99, epsilon=1e-5)\n",
        "\n",
        "    # 4. Backpropagation\n",
        "    _train = trainer.apply_gradients(grads)\n",
        "    \n",
        "    def train(states_in, actions, returns, values, lr):\n",
        "      # Here we calculate advantage A(s,a) = R + yV(s') - V(s)\n",
        "      # Returns = R + yV(s')\n",
        "      advantages = returns - values\n",
        "\n",
        "      # We create the feed dictionary\n",
        "      td_map = {train_model.inputs_: states_in, \n",
        "                actions_: actions,\n",
        "                advantages_: advantages, # Use to calculate our policy loss\n",
        "                rewards_: returns, # Use as a bootstrap for real value\n",
        "                lr_: lr}\n",
        "\n",
        "      policy_loss, value_loss, policy_entropy, _= sess.run([pg_loss, vf_loss, entropy, _train], td_map)\n",
        "\n",
        "      return policy_loss, value_loss, policy_entropy\n",
        "\n",
        "    def save(save_path):\n",
        "      \"\"\"\n",
        "      Save the model\n",
        "      \"\"\"\n",
        "      saver = tf.train.Saver()\n",
        "      saver.save(sess, save_path)\n",
        "\n",
        "    def load(load_path):\n",
        "      \"\"\"\n",
        "      Load the model\n",
        "      \"\"\"\n",
        "      saver = tf.train.Saver()\n",
        "      print('Loading ' + load_path)\n",
        "      saver.restore(sess, load_path)\n",
        "\n",
        "    self.train = train\n",
        "    self.train_model = train_model\n",
        "    self.step_model = step_model\n",
        "    self.step = step_model.step\n",
        "    self.value = step_model.value\n",
        "    self.initial_state = step_model.initial_state\n",
        "    self.save = save\n",
        "    self.load = load\n",
        "    tf.global_variables_initializer().run(session=sess)\n",
        "\n",
        "class Runner(AbstractEnvRunner):\n",
        "  \"\"\"\n",
        "  We use this object to make a mini batch of experiences\n",
        "  __init__:\n",
        "  - Initialize the runner\n",
        "\n",
        "  run():\n",
        "  - Make a mini batch\n",
        "  \"\"\"\n",
        "  def __init__(self, env, model, nsteps, total_timesteps, gamma, lam):    \n",
        "    super().__init__(env = env, model = model, nsteps = nsteps)\n",
        "    \n",
        "    # Discount rate\n",
        "    self.gamma = gamma\n",
        "\n",
        "    # Lambda used in GAE (General Advantage Estimation)\n",
        "    self.lam = lam\n",
        "\n",
        "    # Total timesteps taken\n",
        "    self.total_timesteps = total_timesteps    \n",
        "\n",
        "  def run(self):\n",
        "    # Here, we init the lists that will contain the mb of experiences\n",
        "    mb_obs, mb_actions, mb_rewards, mb_values, mb_dones = [],[],[],[],[]\n",
        "\n",
        "    # For n in range number of steps\n",
        "    for n in range(self.nsteps):\n",
        "      # Given observations, take action and value (V(s))\n",
        "      # We already have self.obs because AbstractEnvRunner run self.obs[:] = env.reset()\n",
        "      actions, values = self.model.step(self.obs, self.dones)\n",
        "\n",
        "      #print(\"actions runner runner\", actions)\n",
        "\n",
        "      # Append the observations into the mb\n",
        "      mb_obs.append(np.copy(self.obs)) #obs len nenvs (1 step per env)\n",
        "\n",
        "      # Append the actions taken into the mb\n",
        "      mb_actions.append(actions)\n",
        "\n",
        "      # Append the values calculated into the mb\n",
        "      mb_values.append(values)\n",
        "\n",
        "      # Append the dones situations into the mb\n",
        "      mb_dones.append(self.dones)\n",
        "\n",
        "      # Take actions in env and look the results\n",
        "      self.obs[:], rewards, self.dones, _ = self.env.step(actions)\n",
        "\n",
        "      mb_rewards.append(rewards)\n",
        "\n",
        "    #batch of steps to batch of rollouts\n",
        "    mb_obs = np.asarray(mb_obs, dtype=np.uint8)\n",
        "    mb_rewards = np.asarray(mb_rewards, dtype=np.float32)\n",
        "    mb_actions = np.asarray(mb_actions, dtype=np.int32)\n",
        "    mb_values = np.asarray(mb_values, dtype=np.float32)\n",
        "    mb_dones = np.asarray(mb_dones, dtype=np.bool)\n",
        "    last_values = self.model.value(self.obs)\n",
        "\n",
        "\n",
        "    ### GENERALIZED ADVANTAGE ESTIMATION\n",
        "    # discount/bootstrap off value fn\n",
        "    # We create mb_returns and mb_advantages\n",
        "    # mb_returns will contain Advantage + value\n",
        "    mb_returns = np.zeros_like(mb_rewards)\n",
        "    mb_advantages = np.zeros_like(mb_rewards)\n",
        "\n",
        "    lastgaelam = 0\n",
        "\n",
        "    # From last step to first step\n",
        "    for t in reversed(range(self.nsteps)):\n",
        "      # If t == before last step\n",
        "      if t == self.nsteps - 1:\n",
        "        # If a state is done, nextnonterminal = 0\n",
        "        # In fact nextnonterminal allows us to do that logic\n",
        "\n",
        "        #if done (so nextnonterminal = 0):\n",
        "        #    delta = R - V(s) (because self.gamma * nextvalues * nextnonterminal = 0)\n",
        "        # else (not done)\n",
        "            #delta = R + gamma * V(st+1)\n",
        "        nextnonterminal = 1.0 - self.dones\n",
        "\n",
        "        # V(t+1)\n",
        "        nextvalues = last_values\n",
        "      else:\n",
        "        nextnonterminal = 1.0 - mb_dones[t+1]\n",
        "\n",
        "        nextvalues = mb_values[t+1]\n",
        "\n",
        "      # Delta = R(st) + gamma * V(t+1) * nextnonterminal  - V(st)\n",
        "      delta = mb_rewards[t] + self.gamma * nextvalues * nextnonterminal - mb_values[t]\n",
        "\n",
        "      # Advantage = delta + gamma *  Î» (lambda) * nextnonterminal  * lastgaelam\n",
        "      mb_advantages[t] = lastgaelam = delta + self.gamma * self.lam * nextnonterminal * lastgaelam\n",
        "\n",
        "    # Returns\n",
        "    mb_returns = mb_advantages + mb_values\n",
        "\n",
        "    return map(sf01, (mb_obs, mb_actions, mb_returns, mb_values))\n",
        "\n",
        "def sf01(arr):\n",
        "  \"\"\"\n",
        "  swap and then flatten axes 0 and 1\n",
        "  \"\"\"\n",
        "  s = arr.shape\n",
        "  return arr.swapaxes(0, 1).reshape(s[0] * s[1], *s[2:])\n",
        "\n",
        "def learn(policy, env, nsteps, total_timesteps, gamma, lam, vf_coef, ent_coef, lr, max_grad_norm, log_interval):\n",
        "    \n",
        "  noptepochs = 4\n",
        "  nminibatches = 8\n",
        "\n",
        "  # Get the number of env\n",
        "  nenvs = env.num_envs\n",
        "\n",
        "  # Get state_space and action_space\n",
        "  ob_space = env.observation_space\n",
        "  ac_space = env.action_space\n",
        "\n",
        "  # Calculate the batch_size\n",
        "  batch_size = nenvs * nsteps # For instance if we take 5 steps and we have 5 environments batch_size = 25\n",
        "\n",
        "  batch_train_size = batch_size // nminibatches\n",
        "\n",
        "  assert batch_size % nminibatches == 0\n",
        "  \n",
        "  # Instantiate the model object (that creates step_model and train_model)\n",
        "  model = Model(policy=policy, ob_space=ob_space, action_space=ac_space, nenvs=nenvs, nsteps=nsteps, \n",
        "                ent_coef=ent_coef, vf_coef=vf_coef, max_grad_norm=max_grad_norm)\n",
        "\n",
        "  # Load the model\n",
        "  # If you want to continue training\n",
        "  if load:\n",
        "    load_path = \"model.ckpt\"\n",
        "    model.load(load_path)\n",
        "  \n",
        "  # Instantiate the runner object\n",
        "  runner = Runner(env, model, nsteps=nsteps, total_timesteps=total_timesteps, gamma=gamma, lam=lam)\n",
        "  \n",
        "  # Index of each element of batch_size\n",
        "  # Create the indices array\n",
        "  indices = np.arange(batch_size)\n",
        "\n",
        "  # Start total timer\n",
        "  tfirststart = time.time()\n",
        "  for update in range(1, total_timesteps//batch_size+1):\n",
        "    # Start timer\n",
        "    tstart = time.time()\n",
        "\n",
        "    # Get minibatch\n",
        "    obs, actions, returns, values = runner.run()\n",
        "\n",
        "    # Here what we're going to do is for each minibatch calculate the loss and append it.\n",
        "    mb_losses = []\n",
        "    total_batches_train = 0\n",
        "\n",
        "    for _ in range(noptepochs):\n",
        "      # Randomize the indexes\n",
        "      np.random.shuffle(indices)\n",
        "\n",
        "      # 0 to batch_size with batch_train_size step\n",
        "      for start in range(0, batch_size, batch_train_size):\n",
        "        end = start + batch_train_size\n",
        "        mbinds = indices[start:end]\n",
        "        slices = (arr[mbinds] for arr in (obs, actions, returns, values))\n",
        "        mb_losses.append(model.train(*slices, lr))\n",
        "\n",
        "    # Feedforward --> get losses --> update\n",
        "    lossvalues = np.mean(mb_losses, axis=0)\n",
        "\n",
        "    # End timer\n",
        "    tnow = time.time()\n",
        "\n",
        "    # Calculate the fps (frame per second)\n",
        "    fps = int(batch_size / (tnow - tstart))\n",
        "    print(update)\n",
        "    if update % log_interval == 0 or update == 1:\n",
        "      \"\"\"\n",
        "      Computes fraction of variance that ypred explains about y.\n",
        "      Returns 1 - Var[y-ypred] / Var[y]\n",
        "      interpretation:\n",
        "      ev=0  =>  might as well have predicted zero\n",
        "      ev=1  =>  perfect prediction\n",
        "      ev<0  =>  worse than just predicting zero\n",
        "      \"\"\"\n",
        "      ev = explained_variance(values, returns)\n",
        "      logger.record_tabular(\"nupdates\", update)\n",
        "      logger.record_tabular(\"total_timesteps\", update*batch_size)\n",
        "      logger.record_tabular(\"fps\", fps)\n",
        "      logger.record_tabular(\"policy_loss\", float(lossvalues[0]))\n",
        "      logger.record_tabular(\"policy_entropy\", float(lossvalues[2]))\n",
        "      logger.record_tabular(\"value_loss\", float(lossvalues[1]))\n",
        "      logger.record_tabular(\"explained_variance\", float(ev))\n",
        "      logger.record_tabular(\"time elapsed\", float(tnow - tfirststart))\n",
        "      logger.dump_tabular()\n",
        "\n",
        "      savepath = \"model.ckpt\"\n",
        "      model.save(savepath)\n",
        "      print('Saving to', savepath)\n",
        "\n",
        "  env.close()\n",
        "\n",
        "def play(policy, env):\n",
        "  # Get state_space and action_space\n",
        "  ob_space = env.observation_space\n",
        "  ac_space = env.action_space\n",
        "  \n",
        "  # Instantiate the model object (that creates step_model and train_model)\n",
        "  model = Model(policy=policy, ob_space=ob_space, action_space=ac_space,\n",
        "                nenvs=1, nsteps=1, ent_coef=0, vf_coef=0, max_grad_norm=0)\n",
        "\n",
        "  # Load the model\n",
        "  load_path = \"model.ckpt\"\n",
        "  model.load(load_path)    \n",
        "\n",
        "  # Play\n",
        "  obs = env.reset()  \n",
        "  score = 0\n",
        "  done = False\n",
        "  sequence_of_actions = []\n",
        "\n",
        "  while done == False:\n",
        "    # Get the action\n",
        "    actions, values = model.step(obs)\n",
        "\n",
        "    # Take actions in env and look the results\n",
        "    obs, rewards, done, _ = env.step(actions)\n",
        "    sequence_of_actions.append(actions)\n",
        "    score += rewards\n",
        "\n",
        "  print(\"Score \", score)\n",
        "  env.close()\n",
        "  return sequence_of_actions\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBcVSH0VMP6s"
      },
      "source": [
        "load = False\n",
        "def main():\n",
        "  config = tf.ConfigProto()\n",
        "\n",
        "  # Avoid warning message errors\n",
        "  os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "\n",
        "  # Allowing GPU memory growth\n",
        "  config.gpu_options.allow_growth = True\n",
        "\n",
        "  tf.reset_default_graph()\n",
        "\n",
        "  with tf.Session(config=config):\n",
        "    learn(policy=A2CPolicy,\n",
        "    env= SubprocVecEnv([make_train_0, make_train_1, make_train_2, make_train_3, make_train_4, make_train_5, make_train_6, make_train_7]),\n",
        "    nsteps=2048, # Steps per environment\n",
        "    total_timesteps=163840, gamma=0.99, lam = 0.95,\n",
        "    vf_coef=0.5, ent_coef=0.01, lr = 2e-4, max_grad_norm = 0.5, log_interval = 5)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uj1xF38Wcg07"
      },
      "source": [
        "files.download(\"checkpoint\")\n",
        "files.download(\"model.ckpt.data-00000-of-00001\")\n",
        "files.download(\"model.ckpt.index\")\n",
        "files.download(\"model.ckpt.meta\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMiiEypzYwkZ"
      },
      "source": [
        "from baselines.common.vec_env.dummy_vec_env import DummyVecEnv\n",
        "\n",
        "global sequence_of_actions\n",
        "config = tf.ConfigProto()\n",
        "\n",
        "# Avoid warning message errors\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "\n",
        "# Allowing GPU memory growth\n",
        "config.gpu_options.allow_growth = True\n",
        "tf.reset_default_graph()\n",
        "with tf.Session(config=config):\n",
        "  sequence_of_actions = play(policy=A2CPolicy, env= SubprocVecEnv([make_train_4]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8v3pSbwXBbyh"
      },
      "source": [
        "env = make_train_4()\n",
        "env.reset()\n",
        "\n",
        "for i in range(len(sequence_of_actions)):\n",
        "  for i in range(4):\n",
        "    observation, reward, done, info = env.step(sequence_of_actions[i][0])\n",
        "    if(done == True):\n",
        "      break\n",
        "\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7jmDkGOIu1E"
      },
      "source": [
        "!python3 -m retro.scripts.playback_movie /content/SuperMarioBros-Nes-Level5-1-000000.bk2"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}